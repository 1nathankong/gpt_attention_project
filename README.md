# gpt_attention_project
FlashAttention exploration with small LLM's
